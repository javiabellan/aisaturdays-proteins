{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo de Macro</th>\n",
       "      <th>Secuencia</th>\n",
       "      <th>Longitud</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uniprot Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O28751</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MEVLFEAKVGDITLKLAQGDITQYPAKAIVNAANKRLEHGGGVAYA...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3RWS7</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MEVEVVRELEMDKLKVKLAGGDITKYPAEAIVNAANKYLEHGGGVA...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2RH24</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MVVKKFGSVEVVLEKGDITKYPAEAIVNAANKYLEHGGGVALAIAK...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A0F7ICE9</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MKPEVVLRFSGVEVRLVQGDITKYPAEAIVNAANRHLEHGGGVAYA...</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A075LQ95</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MNLTELTFGNLTFKLAQGDITKLPAEAIVNAANKYLEHGGGVALAI...</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tipo de Macro                                          Secuencia  \\\n",
       "Uniprot Code                                                                    \n",
       "O28751        AF-1521-like  MEVLFEAKVGDITLKLAQGDITQYPAKAIVNAANKRLEHGGGVAYA...   \n",
       "D3RWS7        AF-1521-like  MEVEVVRELEMDKLKVKLAGGDITKYPAEAIVNAANKYLEHGGGVA...   \n",
       "D2RH24        AF-1521-like  MVVKKFGSVEVVLEKGDITKYPAEAIVNAANKYLEHGGGVALAIAK...   \n",
       "A0A0F7ICE9    AF-1521-like  MKPEVVLRFSGVEVRLVQGDITKYPAEAIVNAANRHLEHGGGVAYA...   \n",
       "A0A075LQ95    AF-1521-like  MNLTELTFGNLTFKLAQGDITKLPAEAIVNAANKYLEHGGGVALAI...   \n",
       "\n",
       "              Longitud  \n",
       "Uniprot Code            \n",
       "O28751             192  \n",
       "D3RWS7             193  \n",
       "D2RH24             193  \n",
       "A0A0F7ICE9         194  \n",
       "A0A075LQ95         190  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/macros.csv\", index_col=\"Uniprot Code\")\n",
    "df[\"Longitud\"] = df.Secuencia.str.len() # Add lenght column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182079"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_aminoacids = df[\"Longitud\"].sum()\n",
    "total_num_aminoacids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'X': 19,\n",
       " 'Y': 20,\n",
       " '[PAD]': 21}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacids = sorted(list(set(df['Secuencia'].apply(set).apply(list).sum())))\n",
    "aminoacids = aminoacids + [\"[PAD]\"]\n",
    "vocab_size = len(aminoacids)\n",
    "\n",
    "amin_dict = {a: i for i, a in enumerate(aminoacids)}\n",
    "numb_dict = {i: a for i, a in enumerate(aminoacids)}\n",
    "amin_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "Predecir acada aminoacido por sus antariores y posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, n_anteriores=4, n_posteriores=4):\n",
    "        indexes = []\n",
    "        \n",
    "        # Paddin at the beggining\n",
    "        for i in range(n_anteriores):\n",
    "            indexes.append([-1,amin_dict[\"[PAD]\"]])\n",
    "        \n",
    "        # Save [seq_id, aminoacid_number] pairs FOR ALL AMINOACIDS\n",
    "        for i, seq in enumerate(df.Secuencia):\n",
    "            for ami in seq:\n",
    "                indexes.append([i, amin_dict[ami]])\n",
    "\n",
    "        # Padding at the end\n",
    "        for i in range(n_posteriores):\n",
    "            indexes.append([-1,amin_dict[\"[PAD]\"]])\n",
    "    \n",
    "\n",
    "        self.indexes       = np.array(indexes)\n",
    "        self.n_anteriores  = n_anteriores\n",
    "        self.n_posteriores = n_posteriores\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Add anteriores to select correct index\n",
    "        index += self.n_anteriores\n",
    "        \n",
    "        # Select window\n",
    "        window = self.indexes[index-self.n_anteriores:index+self.n_posteriores+1]\n",
    "        \n",
    "        # Find sequence id, and target aminoacid\n",
    "        seq_num = window[self.n_anteriores, 0]\n",
    "        ami_y   = window[self.n_anteriores, 1]\n",
    "        \n",
    "        # Delete target aminoacid from window\n",
    "        window = np.delete(window, self.n_anteriores, axis=0)\n",
    "        \n",
    "        # Padding if some window aminoacid is from another sequence\n",
    "        window[:,1][window[:,0]!=seq_num] = amin_dict[\"[PAD]\"]\n",
    "        \n",
    "        # Select aminacids from window (anteriores y posteriores)\n",
    "        ami_x  = window[:, 1]\n",
    "        \n",
    "        # To tensor\n",
    "        ami_x = torch.tensor(ami_x)\n",
    "        ami_y = torch.tensor(ami_y)\n",
    "        \n",
    "        \n",
    "        return ami_x, ami_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return total_num_aminoacids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([21, 21, 21, 21,  3, 17,  9,  4]), tensor(10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_anteriores  = 4\n",
    "n_posteriores = 4\n",
    "n_inputs      = n_anteriores + n_posteriores\n",
    "emb_size      = 8 # (less than 20, because ther are 20 types of aminoacids)\n",
    "n_hidden      = 100\n",
    "n_output      = vocab_size\n",
    "\n",
    "dataset    = SequencesDataset(n_anteriores, n_posteriores)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LM, self).__init__()\n",
    "        self.E  = nn.Embedding(vocab_size, emb_size)                                   # Embedding\n",
    "        self.W1 = nn.Parameter(torch.randn(n_inputs * emb_size, n_hidden).type(dtype)) # Dense 1 weights\n",
    "        self.B1 = nn.Parameter(torch.randn(n_hidden).type(dtype))                      # Dense 1 bias\n",
    "        self.W2 = nn.Parameter(torch.randn(n_hidden, n_output).type(dtype))            # Dense 2 weights\n",
    "        self.RW = nn.Parameter(torch.randn(n_inputs * emb_size, n_output).type(dtype)) # Dense 2 residual weights\n",
    "        self.B2 = nn.Parameter(torch.randn(n_output).type(dtype))                      # Dense 2 bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.E(X)                       # Embeding layer          [bs, n_inputs,  emb_size]\n",
    "        X = X.view(-1, n_inputs * emb_size) # Embedings concatenation [bs, n_inputs * emb_size]\n",
    "        tanh = torch.tanh(self.B1 + torch.mm(X, self.W1)) # Dense layer 1 [bs, hidden_size]\n",
    "        output = self.B2 + torch.mm(X, self.RW) + torch.mm(tanh, self.W2) # Dense layer 2 with residual [bs, vocab_size]\n",
    "        return output\n",
    "\n",
    "model = LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000 cost = 15.247977\n",
      "Step: 2000 cost = 8.468339\n",
      "Step: 3000 cost = 6.261123\n",
      "Step: 4000 cost = 6.773494\n",
      "Step: 5000 cost = 5.099769\n",
      "Step: 6000 cost = 4.200829\n",
      "Step: 7000 cost = 3.520826\n",
      "Step: 8000 cost = 4.982923\n",
      "Step: 9000 cost = 4.239739\n",
      "Step: 10000 cost = 3.083456\n",
      "Step: 11000 cost = 3.351819\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        inputs, labels = batch      # Get [x,y] data\n",
    "        optimizer.zero_grad()       # Zero the parameter gradients\n",
    "        output = model(inputs)      # Forward -> [batch_size, vocab_size]\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1)%1000 == 0:\n",
    "            print('Step:', '%04d' % (i + 1), 'cost =', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.283717</td>\n",
       "      <td>0.195669</td>\n",
       "      <td>0.473640</td>\n",
       "      <td>0.106540</td>\n",
       "      <td>0.716131</td>\n",
       "      <td>-0.021891</td>\n",
       "      <td>0.142731</td>\n",
       "      <td>0.104561</td>\n",
       "      <td>-0.196755</td>\n",
       "      <td>-0.118723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.160171</td>\n",
       "      <td>0.390802</td>\n",
       "      <td>0.438237</td>\n",
       "      <td>-0.204678</td>\n",
       "      <td>0.592143</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>-0.091551</td>\n",
       "      <td>0.421034</td>\n",
       "      <td>-0.530339</td>\n",
       "      <td>0.035108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.423275</td>\n",
       "      <td>-0.106441</td>\n",
       "      <td>0.344730</td>\n",
       "      <td>0.453353</td>\n",
       "      <td>0.327267</td>\n",
       "      <td>-0.136446</td>\n",
       "      <td>0.356835</td>\n",
       "      <td>0.403207</td>\n",
       "      <td>0.387629</td>\n",
       "      <td>-0.448104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.146869</td>\n",
       "      <td>0.590459</td>\n",
       "      <td>0.459565</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>0.891540</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>0.098626</td>\n",
       "      <td>-0.093883</td>\n",
       "      <td>-0.387901</td>\n",
       "      <td>0.296791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.208693</td>\n",
       "      <td>0.217312</td>\n",
       "      <td>0.509949</td>\n",
       "      <td>-0.068511</td>\n",
       "      <td>0.664986</td>\n",
       "      <td>0.044335</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.277070</td>\n",
       "      <td>-0.373452</td>\n",
       "      <td>-0.107069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.154652</td>\n",
       "      <td>0.713851</td>\n",
       "      <td>0.393957</td>\n",
       "      <td>-0.015624</td>\n",
       "      <td>0.828595</td>\n",
       "      <td>0.218472</td>\n",
       "      <td>-0.045019</td>\n",
       "      <td>-0.043033</td>\n",
       "      <td>-0.512101</td>\n",
       "      <td>0.432557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>-0.326036</td>\n",
       "      <td>1.013176</td>\n",
       "      <td>0.185688</td>\n",
       "      <td>0.814158</td>\n",
       "      <td>0.327890</td>\n",
       "      <td>0.568581</td>\n",
       "      <td>0.697741</td>\n",
       "      <td>-0.252586</td>\n",
       "      <td>0.645722</td>\n",
       "      <td>1.238576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.172370</td>\n",
       "      <td>0.309893</td>\n",
       "      <td>0.488456</td>\n",
       "      <td>0.048835</td>\n",
       "      <td>0.648789</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.181176</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>-0.213514</td>\n",
       "      <td>0.040688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.099727</td>\n",
       "      <td>0.682186</td>\n",
       "      <td>0.483253</td>\n",
       "      <td>0.025245</td>\n",
       "      <td>0.871173</td>\n",
       "      <td>0.168764</td>\n",
       "      <td>-0.010429</td>\n",
       "      <td>-0.086051</td>\n",
       "      <td>-0.487728</td>\n",
       "      <td>0.419298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.560810</td>\n",
       "      <td>-0.228176</td>\n",
       "      <td>0.448011</td>\n",
       "      <td>0.520682</td>\n",
       "      <td>0.808180</td>\n",
       "      <td>-0.345476</td>\n",
       "      <td>0.475233</td>\n",
       "      <td>-0.133881</td>\n",
       "      <td>0.326914</td>\n",
       "      <td>-0.574263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.410990</td>\n",
       "      <td>0.029206</td>\n",
       "      <td>0.228324</td>\n",
       "      <td>0.356980</td>\n",
       "      <td>0.282978</td>\n",
       "      <td>-0.156874</td>\n",
       "      <td>0.340761</td>\n",
       "      <td>0.428329</td>\n",
       "      <td>0.326233</td>\n",
       "      <td>-0.358034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.333292</td>\n",
       "      <td>0.144718</td>\n",
       "      <td>0.486511</td>\n",
       "      <td>0.061980</td>\n",
       "      <td>0.891469</td>\n",
       "      <td>-0.031265</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>0.040166</td>\n",
       "      <td>-0.334824</td>\n",
       "      <td>-0.185360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.484003</td>\n",
       "      <td>-0.124266</td>\n",
       "      <td>0.428886</td>\n",
       "      <td>0.339704</td>\n",
       "      <td>0.677113</td>\n",
       "      <td>-0.227052</td>\n",
       "      <td>0.279392</td>\n",
       "      <td>0.089577</td>\n",
       "      <td>0.124028</td>\n",
       "      <td>-0.490430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.230590</td>\n",
       "      <td>-0.089235</td>\n",
       "      <td>0.305962</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>-0.004799</td>\n",
       "      <td>0.484743</td>\n",
       "      <td>0.478657</td>\n",
       "      <td>0.606663</td>\n",
       "      <td>-0.279378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.221220</td>\n",
       "      <td>0.267773</td>\n",
       "      <td>0.414356</td>\n",
       "      <td>0.214267</td>\n",
       "      <td>0.619342</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.231164</td>\n",
       "      <td>0.174796</td>\n",
       "      <td>-0.041016</td>\n",
       "      <td>-0.012606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.480305</td>\n",
       "      <td>-0.094212</td>\n",
       "      <td>0.366098</td>\n",
       "      <td>0.528945</td>\n",
       "      <td>0.605472</td>\n",
       "      <td>-0.215191</td>\n",
       "      <td>0.438778</td>\n",
       "      <td>0.052946</td>\n",
       "      <td>0.376120</td>\n",
       "      <td>-0.415568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.390713</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.432981</td>\n",
       "      <td>0.288524</td>\n",
       "      <td>0.771418</td>\n",
       "      <td>-0.098058</td>\n",
       "      <td>0.257192</td>\n",
       "      <td>0.036674</td>\n",
       "      <td>0.010481</td>\n",
       "      <td>-0.294851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.411595</td>\n",
       "      <td>0.188985</td>\n",
       "      <td>1.131890</td>\n",
       "      <td>0.055486</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>1.029225</td>\n",
       "      <td>-0.068749</td>\n",
       "      <td>1.270467</td>\n",
       "      <td>0.509529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>0.127479</td>\n",
       "      <td>0.346962</td>\n",
       "      <td>0.360147</td>\n",
       "      <td>-0.317605</td>\n",
       "      <td>0.829740</td>\n",
       "      <td>0.253655</td>\n",
       "      <td>-0.103930</td>\n",
       "      <td>0.194388</td>\n",
       "      <td>-0.693378</td>\n",
       "      <td>0.107867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.472155</td>\n",
       "      <td>0.444354</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.683778</td>\n",
       "      <td>0.118142</td>\n",
       "      <td>0.106468</td>\n",
       "      <td>0.105234</td>\n",
       "      <td>-0.303252</td>\n",
       "      <td>0.203972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "A  0.283717  0.195669  0.473640  0.106540  0.716131 -0.021891  0.142731   \n",
       "C  0.160171  0.390802  0.438237 -0.204678  0.592143  0.140152 -0.091551   \n",
       "D  0.423275 -0.106441  0.344730  0.453353  0.327267 -0.136446  0.356835   \n",
       "E  0.146869  0.590459  0.459565  0.060844  0.891540  0.057335  0.098626   \n",
       "F  0.208693  0.217312  0.509949 -0.068511  0.664986  0.044335  0.031112   \n",
       "G  0.154652  0.713851  0.393957 -0.015624  0.828595  0.218472 -0.045019   \n",
       "H -0.326036  1.013176  0.185688  0.814158  0.327890  0.568581  0.697741   \n",
       "I  0.172370  0.309893  0.488456  0.048835  0.648789  0.067857  0.181176   \n",
       "K  0.099727  0.682186  0.483253  0.025245  0.871173  0.168764 -0.010429   \n",
       "L  0.560810 -0.228176  0.448011  0.520682  0.808180 -0.345476  0.475233   \n",
       "M  0.410990  0.029206  0.228324  0.356980  0.282978 -0.156874  0.340761   \n",
       "N  0.333292  0.144718  0.486511  0.061980  0.891469 -0.031265  0.064922   \n",
       "P  0.484003 -0.124266  0.428886  0.339704  0.677113 -0.227052  0.279392   \n",
       "Q  0.230590 -0.089235  0.305962  0.530781  0.087146 -0.004799  0.484743   \n",
       "R  0.221220  0.267773  0.414356  0.214267  0.619342  0.018936  0.231164   \n",
       "S  0.480305 -0.094212  0.366098  0.528945  0.605472 -0.215191  0.438778   \n",
       "T  0.390713  0.006897  0.432981  0.288524  0.771418 -0.098058  0.257192   \n",
       "V  0.037519  0.411595  0.188985  1.131890  0.055486  0.221900  1.029225   \n",
       "W  0.127479  0.346962  0.360147 -0.317605  0.829740  0.253655 -0.103930   \n",
       "Y  0.134700  0.472155  0.444354  0.030612  0.683778  0.118142  0.106468   \n",
       "\n",
       "          7      pca1      pca2  \n",
       "A  0.104561 -0.196755 -0.118723  \n",
       "C  0.421034 -0.530339  0.035108  \n",
       "D  0.403207  0.387629 -0.448104  \n",
       "E -0.093883 -0.387901  0.296791  \n",
       "F  0.277070 -0.373452 -0.107069  \n",
       "G -0.043033 -0.512101  0.432557  \n",
       "H -0.252586  0.645722  1.238576  \n",
       "I  0.156057 -0.213514  0.040688  \n",
       "K -0.086051 -0.487728  0.419298  \n",
       "L -0.133881  0.326914 -0.574263  \n",
       "M  0.428329  0.326233 -0.358034  \n",
       "N  0.040166 -0.334824 -0.185360  \n",
       "P  0.089577  0.124028 -0.490430  \n",
       "Q  0.478657  0.606663 -0.279378  \n",
       "R  0.174796 -0.041016 -0.012606  \n",
       "S  0.052946  0.376120 -0.415568  \n",
       "T  0.036674  0.010481 -0.294851  \n",
       "V -0.068749  1.270467  0.509529  \n",
       "W  0.194388 -0.693378  0.107867  \n",
       "Y  0.105234 -0.303252  0.203972  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = pd.DataFrame(data=model.E.weight.data.numpy(), index=aminoacids)\n",
    "emb_df = emb_df.drop([\"X\", \"[PAD]\"])\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit_transform(emb_df.to_numpy())  # Compute PCA\n",
    "emb_df['pca1'] = pca[:,0]\n",
    "emb_df['pca2'] = pca[:,1]\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-30e8c94b7c824c92b028d1cfd04bd430\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-30e8c94b7c824c92b028d1cfd04bd430\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-30e8c94b7c824c92b028d1cfd04bd430\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"circle\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"pca1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pca2\"}}}, {\"mark\": {\"type\": \"text\", \"dx\": 10}, \"encoding\": {\"text\": {\"type\": \"nominal\", \"field\": \"index\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"pca1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pca2\"}}}], \"data\": {\"name\": \"data-5d0f7899b12da4dae03a8e3391dd550f\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-5d0f7899b12da4dae03a8e3391dd550f\": [{\"index\": \"A\", \"pca1\": -0.19675493240356445, \"pca2\": -0.11872293800115585}, {\"index\": \"C\", \"pca1\": -0.5303393006324768, \"pca2\": 0.03510820120573044}, {\"index\": \"D\", \"pca1\": 0.38762885332107544, \"pca2\": -0.4481043517589569}, {\"index\": \"E\", \"pca1\": -0.38790062069892883, \"pca2\": 0.2967911660671234}, {\"index\": \"F\", \"pca1\": -0.3734515607357025, \"pca2\": -0.10706879943609238}, {\"index\": \"G\", \"pca1\": -0.512100875377655, \"pca2\": 0.43255674839019775}, {\"index\": \"H\", \"pca1\": 0.6457218527793884, \"pca2\": 1.23857581615448}, {\"index\": \"I\", \"pca1\": -0.21351413428783417, \"pca2\": 0.04068809375166893}, {\"index\": \"K\", \"pca1\": -0.48772791028022766, \"pca2\": 0.41929781436920166}, {\"index\": \"L\", \"pca1\": 0.3269144892692566, \"pca2\": -0.5742633938789368}, {\"index\": \"M\", \"pca1\": 0.32623326778411865, \"pca2\": -0.35803353786468506}, {\"index\": \"N\", \"pca1\": -0.3348241150379181, \"pca2\": -0.1853604018688202}, {\"index\": \"P\", \"pca1\": 0.12402764707803726, \"pca2\": -0.4904298186302185}, {\"index\": \"Q\", \"pca1\": 0.6066625714302063, \"pca2\": -0.279378205537796}, {\"index\": \"R\", \"pca1\": -0.04101571813225746, \"pca2\": -0.012605748139321804}, {\"index\": \"S\", \"pca1\": 0.3761204183101654, \"pca2\": -0.4155675172805786}, {\"index\": \"T\", \"pca1\": 0.010481095872819424, \"pca2\": -0.29485127329826355}, {\"index\": \"V\", \"pca1\": 1.2704674005508423, \"pca2\": 0.5095290541648865}, {\"index\": \"W\", \"pca1\": -0.6933776140213013, \"pca2\": 0.10786686092615128}, {\"index\": \"Y\", \"pca1\": -0.3032515347003937, \"pca2\": 0.2039722204208374}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "base = alt.Chart(emb_df[['pca1','pca2']].reset_index()).encode(\n",
    "    x='pca1', y='pca2'\n",
    ")\n",
    "\n",
    "base.mark_circle() + base.mark_text(dx=10).encode(text='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "# Test\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
