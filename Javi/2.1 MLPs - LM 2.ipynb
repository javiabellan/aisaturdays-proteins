{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo de Macro</th>\n",
       "      <th>Secuencia</th>\n",
       "      <th>Longitud</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uniprot Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O28751</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MEVLFEAKVGDITLKLAQGDITQYPAKAIVNAANKRLEHGGGVAYA...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3RWS7</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MEVEVVRELEMDKLKVKLAGGDITKYPAEAIVNAANKYLEHGGGVA...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2RH24</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MVVKKFGSVEVVLEKGDITKYPAEAIVNAANKYLEHGGGVALAIAK...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A0F7ICE9</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MKPEVVLRFSGVEVRLVQGDITKYPAEAIVNAANRHLEHGGGVAYA...</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A075LQ95</th>\n",
       "      <td>AF-1521-like</td>\n",
       "      <td>MNLTELTFGNLTFKLAQGDITKLPAEAIVNAANKYLEHGGGVALAI...</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tipo de Macro                                          Secuencia  \\\n",
       "Uniprot Code                                                                    \n",
       "O28751        AF-1521-like  MEVLFEAKVGDITLKLAQGDITQYPAKAIVNAANKRLEHGGGVAYA...   \n",
       "D3RWS7        AF-1521-like  MEVEVVRELEMDKLKVKLAGGDITKYPAEAIVNAANKYLEHGGGVA...   \n",
       "D2RH24        AF-1521-like  MVVKKFGSVEVVLEKGDITKYPAEAIVNAANKYLEHGGGVALAIAK...   \n",
       "A0A0F7ICE9    AF-1521-like  MKPEVVLRFSGVEVRLVQGDITKYPAEAIVNAANRHLEHGGGVAYA...   \n",
       "A0A075LQ95    AF-1521-like  MNLTELTFGNLTFKLAQGDITKLPAEAIVNAANKYLEHGGGVALAI...   \n",
       "\n",
       "              Longitud  \n",
       "Uniprot Code            \n",
       "O28751             192  \n",
       "D3RWS7             193  \n",
       "D2RH24             193  \n",
       "A0A0F7ICE9         194  \n",
       "A0A075LQ95         190  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/macros.csv\", index_col=\"Uniprot Code\")\n",
    "df[\"Longitud\"] = df.Secuencia.str.len() # Add lenght column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182079"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_aminoacids = df[\"Longitud\"].sum()\n",
    "total_num_aminoacids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'X': 19,\n",
       " 'Y': 20,\n",
       " '[PAD]': 21,\n",
       " '[CLS]': 22,\n",
       " '[SEP]': 23,\n",
       " '[MASK]': 24}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacids = sorted(list(set(df['Secuencia'].apply(set).apply(list).sum())))\n",
    "aminoacids = aminoacids + [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "vocab_size = len(tokens)\n",
    "\n",
    "amin_dict = {a: i for i, a in enumerate(aminoacids)}\n",
    "numb_dict = {i: a for i, a in enumerate(aminoacids)}\n",
    "amin_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesDataset(data.Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images_fn = images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global images\n",
    "        file1 = images[self.images_fn[index][0]]\n",
    "        file2 = images[self.images_fn[index][1]]\n",
    "        val = self.images_fn[index][2]\n",
    "        files = [file1, file2]\n",
    "        return files, val\n",
    "    \n",
    "    def __len__(self):\n",
    "        return total_num_aminoacids\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "        ImagesFromList(images=trainset),\n",
    "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True, collate_fn = my_collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.241908\n",
      "Epoch: 2000 cost = 0.033025\n",
      "Epoch: 3000 cost = 0.010251\n",
      "Epoch: 4000 cost = 0.004399\n",
      "Epoch: 5000 cost = 0.002176\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['dog', 'coffee', 'milk']\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "emb_size = 8\n",
    "n_step   = 2\n",
    "n_hidden = 2 # h in paper\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "# Model\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LM, self).__init__()\n",
    "        self.E  = nn.Embedding(n_class, emb_size)                                     # Embedding\n",
    "        self.W1 = nn.Parameter(torch.randn(n_step * emb_size, n_hidden).type(dtype))  # Dense 1 weights\n",
    "        self.B1 = nn.Parameter(torch.randn(n_hidden).type(dtype))                     # Dense 1 bias\n",
    "        self.W2 = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))            # Dense 2 weights\n",
    "        self.RW = nn.Parameter(torch.randn(n_step * emb_size, n_class).type(dtype))   # Dense 2 residual weights\n",
    "        self.B2 = nn.Parameter(torch.randn(n_class).type(dtype))                      # Dense 2 bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.E(X)                     # Embeding layer          [bs, n_step,  emb_size]\n",
    "        X = X.view(-1, n_step * emb_size) # Embedings concatenation [bs, n_step * emb_size]\n",
    "        tanh = torch.tanh(self.B1 + torch.mm(X, self.W1)) # Dense layer 1 [bs, hidden_size]\n",
    "        output = self.B2 + torch.mm(X, self.RW) + torch.mm(tanh, self.W2) # Dense layer 2 with residual [bs, vocab_size]\n",
    "        return output\n",
    "\n",
    "model = LM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = Variable(torch.LongTensor(input_batch))\n",
    "target_batch = Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "# Training\n",
    "for epoch in range(5000):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1)%1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "# Test\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
